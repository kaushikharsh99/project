"""
LM Studio Public API + Chat UI Script
-------------------------------------
This script connects to your LM Studio model running locally
and creates a simple web chat interface you can share via ngrok.

ğŸ‘‰ Before running:
1ï¸âƒ£ Make sure LM Studio is open and "Enable Local API Server" is ON.
2ï¸âƒ£ Check your LM Studio API URL (default: http://localhost:1234/v1).
3ï¸âƒ£ Install dependencies:
      pip install gradio requests pyngrok
"""

import gradio as gr
import requests
from pyngrok import ngrok

# =====================================
# ğŸ”§ SETTINGS (CHANGE THESE IF NEEDED)
# =====================================

LMSTUDIO_API = "http://localhost:1234/v1/chat/completions"  # ğŸ”§ Change this if LM Studio uses a different port (check in Settings â†’ Developer)
MODEL_NAME = "google/gemma-3n-e4b"  # ğŸ”§ Change to whatever model you loaded in LM Studio
PORT = 7860                                                  # ğŸ”§ Change only if this port is already in use
USE_NGROK = True                                             # ğŸ”§ Set to False if you want LAN-only access (no public link)
USERNAME = "user"                                            # ğŸ”§ Optional: login username
PASSWORD = "1234"                                            # ğŸ”§ Optional: login password (set None to disable auth)

# =====================================
# ğŸ§  START NGROK (for public sharing)
# =====================================
if USE_NGROK:
    print("ğŸš€ Starting ngrok tunnel...")
    public_url = ngrok.connect(PORT)
    print(f"ğŸ”— Public link: {public_url}")
else:
    print("ğŸŒ Running on local Wi-Fi only (no ngrok)")

# =====================================
# ğŸ¤– FUNCTION TO CALL LM STUDIO API
# =====================================
def chat_with_model(prompt, history=[]):
    """
    Sends chat history + user prompt to LM Studio's local API
    and returns the model's reply.
    """
    messages = [{"role": "system", "content": "You are a helpful AI assistant."}]
    for user, bot in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": bot})
    messages.append({"role": "user", "content": prompt})

    payload = {"model": MODEL_NAME, "messages": messages}

    try:
        response = requests.post(LMSTUDIO_API, json=payload, timeout=120)
        response.raise_for_status()
        reply = response.json()["choices"][0]["message"]["content"]
        history.append((prompt, reply))
        return history, history
    except Exception as e:
        error_msg = f"âŒ Error connecting to LM Studio API: {str(e)}"
        history.append((prompt, error_msg))
        return history, history

# =====================================
# ğŸ’¬ CREATE GRADIO WEB CHAT INTERFACE
# =====================================
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– My Local LM Studio Model Chat")
    chatbot = gr.Chatbot(height=400)
    msg = gr.Textbox(label="Enter your message:")
    clear = gr.Button("ğŸ§¹ Clear Chat")

    state = gr.State([])
    msg.submit(chat_with_model, [msg, state], [chatbot, state])
    clear.click(lambda: None, None, chatbot)

# =====================================
# ğŸ START THE WEB APP
# =====================================
auth = (USERNAME, PASSWORD) if USERNAME and PASSWORD else None

demo.launch(
    server_name="0.0.0.0",  # allows local + LAN access
    server_port=PORT,
    auth=auth
)
